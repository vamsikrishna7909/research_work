{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b5f6461-c674-496d-80be-8ef4e1ac8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a512af6f-024d-4969-a6c6-351c50202dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA L40\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b307e5fa-2381-4f54-9d11-0237182b350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "# Define the Label class\n",
    "Label = namedtuple('Label', ['name', 'id', 'trainId', 'category', 'catId', 'hasInstances', 'ignoreInEval', 'color'])\n",
    "\n",
    "# List of Cityscapes labels\n",
    "labels = [\n",
    "    Label('unlabeled', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "    Label('ego vehicle', 1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "    Label('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "    Label('out of roi', 3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "    Label('static', 4, 255, 'void', 0, False, True, (111, 74, 0)),\n",
    "    Label('dynamic', 5, 255, 'void', 0, False, True, (81, 0, 81)),\n",
    "    Label('ground', 6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
    "    Label('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
    "    Label('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
    "    Label('parking', 9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
    "    Label('rail track', 10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
    "    Label('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
    "    Label('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
    "    Label('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
    "    Label('guard rail', 14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
    "    Label('bridge', 15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
    "    Label('tunnel', 16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
    "    Label('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
    "    Label('polegroup', 18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
    "    Label('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
    "    Label('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
    "    Label('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
    "    Label('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
    "    Label('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
    "    Label('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
    "    Label('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
    "    Label('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
    "    Label('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
    "    Label('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
    "    Label('caravan', 29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
    "    Label('trailer', 30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
    "    Label('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
    "    Label('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
    "    Label('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
    "    Label('license plate', -1, -1, 'vehicle', 7, False, True, (0, 0, 142)),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa303b79-8ade-4634-8b02-3792452b3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from class name to trainId\n",
    "name_to_trainId = {label.name: label.trainId for label in labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8220ce10-950c-4b24-ad46-8c601a7562e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d153c8b-a61f-4706-8e5e-f39ec95d5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/jovyan/shared/siglip_task\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09078138-595a-4fde-beda-baa17c59df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify your directories\n",
    "images_folder = \"/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train\"\n",
    "captions_folder = \"/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba6fa89b-765e-48d2-9ea9-70c6be8831bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to recursively collect file paths\n",
    "def collect_files(folder, extension):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return sorted(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d73ae4a-3e7e-41ba-bb7a-20a1e9fd51d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect image and caption files\n",
    "image_files = collect_files(images_folder, '.png')[:1000]\n",
    "caption_files = collect_files(captions_folder, '.txt')[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5523bdd-1146-44ba-ae59-22281b0db154",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32  # Adjust based on your available memory\n",
    "num_batches = len(image_files) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb89f4af-3549-491b-9cb0-f0948503d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Process and encode the first 10,000 images and captions\n",
    "batch_size = 32  # Adjust based on your available memory\n",
    "image_files = sorted([f for f in os.listdir(images_folder) if os.path.isfile(os.path.join(images_folder, f)) and f.endswith('.png')])[:1000]\n",
    "caption_files = sorted([f for f in os.listdir(captions_folder) if os.path.isfile(os.path.join(captions_folder, f)) and f.endswith('.txt')])[:1000]'''\n",
    "\n",
    "if len(image_files) != len(caption_files):\n",
    "    raise ValueError(\"The number of images and captions do not match.\")\n",
    "\n",
    "num_batches = len(image_files) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8b47576-dca5-4f05-a206-ac12f844b90f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize global lists for captions and image tensors\n",
    "all_captions = []\n",
    "all_image_tensors = []\n",
    "all_image_files = []\n",
    "\n",
    "image_features_list = []\n",
    "text_features_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c053360-668e-4301-9d7e-a4e727021925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define binary cross-entropy loss with logits\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a8ccc2d-92ef-4dd4-903f-a8e8134a46a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1/31\n",
      "Batch image files: ['/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/.ipynb_checkpoints/aachen_000000_000019_leftImg8bit-checkpoint.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/.ipynb_checkpoints/aachen_000001_000019_leftImg8bit-checkpoint.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/.ipynb_checkpoints/aachen_000003_000019_leftImg8bit-checkpoint.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/.ipynb_checkpoints/aachen_000008_000019_leftImg8bit-checkpoint.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/.ipynb_checkpoints/aachen_000015_000019_leftImg8bit-checkpoint.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000000_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000001_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000002_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000003_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000004_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000005_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000006_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000007_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000008_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000009_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000010_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000011_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000012_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000013_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000014_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000015_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000016_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000017_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000018_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000019_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000020_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000021_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000022_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000023_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000024_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000025_000019_leftImg8bit.png', '/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train/aachen/aachen_000026_000019_leftImg8bit.png']\n",
      "Batch caption files: ['/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/.ipynb_checkpoints/aachen_000011_000019_leftImg8bit-checkpoint.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/.ipynb_checkpoints/aachen_000018_000019_leftImg8bit-checkpoint.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000000_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000001_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000002_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000003_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000004_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000005_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000006_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000007_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000008_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000009_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000010_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000011_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000012_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000013_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000014_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000015_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000016_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000017_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000018_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000019_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000020_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000021_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000022_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000023_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000024_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000025_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000026_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000027_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000028_000019_leftImg8bit.txt', '/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train/aachen/aachen_000029_000019_leftImg8bit.txt']\n",
      "Before encoding - Batch 1:\n",
      "Image tensors: 32, Captions: 32\n",
      "After encoding - Batch 1:\n",
      "Image embeddings shape: torch.Size([32, 512])\n",
      "Text embeddings shape: torch.Size([32, 512])\n",
      "After normalization - Batch 1:\n",
      "Normalized image embeddings shape: torch.Size([32, 512])\n",
      "Normalized text embeddings shape: torch.Size([32, 512])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatch in target label size (35) and logits size (32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Resize target labels to match the output logits shape\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m logits_per_image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in target label size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and logits size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogits_per_image\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m loss_image \u001b[38;5;241m=\u001b[39m criterion(logits_per_image, target_labels)\n\u001b[1;32m     92\u001b[0m loss_text \u001b[38;5;241m=\u001b[39m criterion(logits_per_text, target_labels)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatch in target label size (35) and logits size (32)"
     ]
    }
   ],
   "source": [
    "for i in range(num_batches):\n",
    "    batch_image_files = image_files[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_caption_files = caption_files[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "    # Debug: Print the current batch files\n",
    "    print(f\"Processing batch {i + 1}/{num_batches}\")\n",
    "    print(f\"Batch image files: {batch_image_files}\")\n",
    "    print(f\"Batch caption files: {batch_caption_files}\")\n",
    "\n",
    "    # Load and process images\n",
    "    image_tensors = []\n",
    "    for img_file in batch_image_files:\n",
    "        try:\n",
    "            image = Image.open(os.path.join(images_folder, img_file)).convert(\"RGB\")\n",
    "            image_tensors.append(image)\n",
    "            all_image_files.append(img_file)  # Store the image file name\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "\n",
    "    # Load and process captions\n",
    "    captions = []\n",
    "    for cap_file in batch_caption_files:\n",
    "        try:\n",
    "            caption = open(os.path.join(captions_folder, cap_file)).read().strip()\n",
    "            captions.append(caption)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading caption {cap_file}: {e}\")\n",
    "\n",
    "    # Append to global lists\n",
    "    all_captions.extend(captions)\n",
    "    all_image_tensors.extend(image_tensors)\n",
    "    \n",
    "    # Print the features before encoding\n",
    "    print(f\"Before encoding - Batch {i + 1}:\")\n",
    "    print(f\"Image tensors: {len(image_tensors)}, Captions: {len(captions)}\")\n",
    "\n",
    "    # Process batch if all files were loaded successfully\n",
    "    if len(image_tensors) == len(batch_image_files) and len(captions) == len(batch_caption_files):\n",
    "        inputs = processor(text=captions, images=image_tensors, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move inputs to the device (GPU or CPU)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass through CLIP model\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Print the features after encoding\n",
    "        print(f\"After encoding - Batch {i + 1}:\")\n",
    "        print(f\"Image embeddings shape: {outputs.image_embeds.shape}\")\n",
    "        print(f\"Text embeddings shape: {outputs.text_embeds.shape}\")\n",
    "\n",
    "        # Normalize the embeddings\n",
    "        image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "        text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Print the features after normalization\n",
    "        print(f\"After normalization - Batch {i + 1}:\")\n",
    "        print(f\"Normalized image embeddings shape: {image_embeds.shape}\")\n",
    "        print(f\"Normalized text embeddings shape: {text_embeds.shape}\")\n",
    "\n",
    "        # Extract features\n",
    "        image_features = outputs.image_embeds\n",
    "        text_features = outputs.text_embeds\n",
    "    \n",
    "\n",
    "        # Move outputs to CPU and store them\n",
    "        image_features_list.append(image_embeds.cpu())\n",
    "        text_features_list.append(text_embeds.cpu())\n",
    "\n",
    "        # Assume binary labels based on the presence of specific keywords in captions\n",
    "        target_labels = []\n",
    "        for caption in captions:\n",
    "            # Initialize an all-zero label vector for each caption\n",
    "            label_vector = torch.zeros(len(name_to_trainId))\n",
    "            for name, trainId in name_to_trainId.items():\n",
    "                if name in caption and trainId != 255:\n",
    "                    label_vector[trainId] = 1\n",
    "            target_labels.append(label_vector)\n",
    "    \n",
    "        target_labels = torch.stack(target_labels).to(device)\n",
    "\n",
    "        # Compute logits and loss\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "\n",
    "        # Resize target labels to match the output logits shape\n",
    "        if target_labels.size(1) != logits_per_image.size(1):\n",
    "            raise ValueError(f\"Mismatch in target label size ({target_labels.size(1)}) and logits size ({logits_per_image.size(1)})\")\n",
    "\n",
    "        loss_image = criterion(logits_per_image, target_labels)\n",
    "        loss_text = criterion(logits_per_text, target_labels)\n",
    "    \n",
    "        # Backward pass (if training)\n",
    "        loss_image.backward()\n",
    "        loss_text.backward()\n",
    "    \n",
    "        print(f\"Processed batch {i + 1}/{num_batches}\")\n",
    "    else:\n",
    "        print(\"Skipping batch due to errors in loading files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663d8f2-968a-4e2b-a24f-165920aa73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate features from all batches\n",
    "image_features = torch.cat(image_features_list, dim=0)\n",
    "text_features = torch.cat(text_features_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f383582-2004-4896-8cf4-7f9d7fa07fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the concatenated features' dimensions\n",
    "print(f\"Concatenated image features shape: {image_features.shape}\")\n",
    "print(f\"Concatenated text features shape: {text_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8616e4a-8aeb-41c4-88c3-265a585e5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigmoid logits and binary cross-entropy loss\n",
    "logits = torch.matmul(image_features, text_features.T)\n",
    "targets = torch.eye(len(image_features)).to(logits.device)  # Dummy targets for demonstration (identity matrix)\n",
    "loss = criterion(logits, targets)\n",
    "\n",
    "# Print the computed loss\n",
    "print(f\"Computed binary cross-entropy loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f87d134-b392-4317-8d84-211a0a8145ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sigmoid probabilities\n",
    "sigmoid_probs = torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd0eb9-4b7c-4f96-aa82-294b9f558012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print shapes of logits and sigmoid probabilities\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Sigmoid probabilities shape: {sigmoid_probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4a11-1a2b-4357-80b3-ba8c2740b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities for comparison\n",
    "cosine_similarities = F.cosine_similarity(\n",
    "    image_features.unsqueeze(1),  # (num_images, 1, feature_dim)\n",
    "    text_features.unsqueeze(0),  # (1, num_captions, feature_dim)\n",
    "    dim=-1  # Compute similarity along the feature dimension\n",
    ")\n",
    "\n",
    "# Print shape of cosine_similarities\n",
    "print(f\"Cosine similarities shape: {cosine_similarities.shape}\")\n",
    "print(f\"Cosine similarities: {cosine_similarities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a7dd9-f6a3-4472-9d87-e43072cdb5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_path):\n",
    "    image = Image.open(os.path.join(images_folder, image_path))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820263a1-4431-4426-94d6-95a0e9bde80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the most similar caption for a given image index\n",
    "def find_most_similar_caption(image_idx):\n",
    "    similarity_scores = cosine_similarities[image_idx]\n",
    "    most_similar_caption_idx = torch.argmax(similarity_scores).item()\n",
    "    print(f\"Image index: {image_idx}, Most similar caption index: {most_similar_caption_idx}, Total captions: {len(all_captions)}\")\n",
    "    return all_captions[most_similar_caption_idx]\n",
    "\n",
    "# Example usage\n",
    "image_idx = 450  # Index of the image you want to query\n",
    "\n",
    "# Show the original image\n",
    "print(\"Original image:\")\n",
    "show_image(all_image_files[image_idx])\n",
    "\n",
    "# Print the original caption and the most similar caption\n",
    "print(\"Original caption:\", all_captions[image_idx])\n",
    "print(\"len of all captions\", len(all_captions))\n",
    "print(\"Most similar caption:\", find_most_similar_caption(image_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec3000-6aa8-4c51-b5d0-a58edaa10950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_image(caption_idx):\n",
    "    similarity_scores = cosine_similarities[:, caption_idx]\n",
    "    most_similar_image_idx = torch.argmax(similarity_scores).item()\n",
    "    print(f\"Caption index: {caption_idx}, Most similar image index: {most_similar_image_idx}, Total images: {len(all_image_files)}\")\n",
    "    return all_image_files[most_similar_image_idx], most_similar_image_idx\n",
    "\n",
    "caption_idx = 450  # Index of the caption you want to query\n",
    "\n",
    "# Print the caption first\n",
    "print(f\"Original caption: {all_captions[caption_idx]}\")\n",
    "\n",
    "# Find and show the most similar image for the given caption\n",
    "most_similar_image, most_similar_image_idx = find_most_similar_image(caption_idx)\n",
    "print(\"Most similar image for the caption:\")\n",
    "show_image(most_similar_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dfe11-ac49-4196-9ec2-51ce4f199ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
