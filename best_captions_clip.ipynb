{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86020382-b269-427d-a57d-437667f92168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import gc\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa01a1d-626a-4e8f-b66f-5187af98d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(filename='processing.log', level=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea41563-b59c-4325-a224-89bd99f8a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e653debc-e208-47e2-acd1-5da10d832a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0edab6-76e1-4af7-8e26-4f2bafd30a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "directories = {\n",
    "    \"gta\": {\n",
    "        \"images\": \"/home/jovyan/shared/dataset/data_seg/gta/images\",\n",
    "        \"captions\": \"/home/jovyan/shared/dataset/data_seg/gta/captions\",\n",
    "        \"output\": \"/home/jovyan/shared/dataset/data_seg/gta/clip_captions\"\n",
    "    },\n",
    "    \"cityscapes_train\": {\n",
    "        \"images\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/train\",\n",
    "        \"captions\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/captions/train\",\n",
    "        \"output\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/clip_captions/train\"\n",
    "    },\n",
    "    \"cityscapes_test\": {\n",
    "        \"images\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/test\",\n",
    "        \"captions\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/captions/test\",\n",
    "        \"output\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/clip_captions/test\"\n",
    "    },\n",
    "    \"cityscapes_val\": {\n",
    "        \"images\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/leftImg8bit/val\",\n",
    "        \"captions\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/captions/val\",\n",
    "        \"output\": \"/home/jovyan/shared/dataset/data_seg/cityscapes/clip_captions/val\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9656631-e92c-4568-b3e7-ae42ad6875f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to recursively collect file paths\n",
    "def collect_files(folder, extension):\n",
    "    file_paths = []\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        if '.ipynb_checkpoints' in root:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return sorted(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c59910-b173-4b85-9847-56bdb82f683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect image and caption files\n",
    "def process_folder(images_folder, captions_folder):\n",
    "    image_files = collect_files(images_folder, '.png')\n",
    "    caption_files = collect_files(captions_folder, '.txt')\n",
    "    return image_files, caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98969cd7-b6e5-4034-9ec6-ee5d288009e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each directory and get image and caption files\n",
    "dataset_files = {key: process_folder(value['images'], value['captions']) for key, value in directories.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446154ec-ebd0-4255-b7fc-56daf60710c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find mismatched files\n",
    "def find_mismatched_files(image_files, caption_files):\n",
    "    image_basenames = {os.path.splitext(os.path.basename(f))[0] for f in image_files}\n",
    "    caption_basenames = {os.path.splitext(os.path.basename(f))[0] for f in caption_files}\n",
    "    \n",
    "    missing_captions = image_basenames - caption_basenames\n",
    "    missing_images = caption_basenames - image_basenames\n",
    "    \n",
    "    missing_caption_files = [f for f in image_files if os.path.splitext(os.path.basename(f))[0] in missing_captions]\n",
    "    missing_image_files = [f for f in caption_files if os.path.splitext(os.path.basename(f))[0] in missing_images]\n",
    "    \n",
    "    return missing_caption_files, missing_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f698e4-3baf-459e-80d3-3223f763c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for mismatched files in each dataset\n",
    "for key, value in directories.items():\n",
    "    image_files, caption_files = dataset_files[key]\n",
    "    missing_caption_files, missing_image_files = find_mismatched_files(image_files, caption_files)\n",
    "    \n",
    "    if missing_caption_files:\n",
    "        print(f\"Missing captions for {len(missing_caption_files)} images in {key}:\")\n",
    "        for f in missing_caption_files:\n",
    "            print(f\"  {f}\")\n",
    "    \n",
    "    if missing_image_files:\n",
    "        print(f\"Missing images for {len(missing_image_files)} captions in {key}:\")\n",
    "        for f in missing_image_files:\n",
    "            print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce794e4-5b52-4b7d-b9ec-15303ca7136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, only process files that have matching pairs\n",
    "def get_matching_files(image_files, caption_files):\n",
    "    image_basenames = {os.path.splitext(os.path.basename(f))[0]: f for f in image_files}\n",
    "    caption_basenames = {os.path.splitext(os.path.basename(f))[0]: f for f in caption_files}\n",
    "    \n",
    "    common_basenames = image_basenames.keys() & caption_basenames.keys()\n",
    "    \n",
    "    matched_image_files = [image_basenames[bn] for bn in common_basenames]\n",
    "    matched_caption_files = [caption_basenames[bn] for bn in common_basenames]\n",
    "    \n",
    "    return matched_image_files, matched_caption_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c11767-a9a3-478f-95a6-32e6e9409388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store the best captions\n",
    "def save_best_captions(best_captions, image_files, save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    for caption, img_file in zip(best_captions, image_files):\n",
    "        img_filename = os.path.splitext(os.path.basename(img_file))[0]\n",
    "        caption_file = os.path.join(save_folder, f\"{img_filename}.txt\")\n",
    "        with open(caption_file, 'w') as file:\n",
    "            file.write(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f211b4-380c-4d26-b5bc-9325a2fd372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process batches of images and captions\n",
    "def process_batches(image_files, caption_files, batch_size):\n",
    "    if len(image_files) != len(caption_files):\n",
    "        raise ValueError(\"The number of images and captions do not match.\")\n",
    "    \n",
    "    num_batches = (len(image_files) + batch_size - 1) // batch_size\n",
    "    image_features_list = []\n",
    "    text_features_list = []\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_image_files = image_files[i * batch_size: (i + 1) * batch_size]\n",
    "        batch_caption_files = caption_files[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "        print(f\"Processing batch {i + 1}/{num_batches}\")\n",
    "        print(f\"Batch image files: {batch_image_files}\")\n",
    "        print(f\"Batch caption files: {batch_caption_files}\")\n",
    "\n",
    "        # Load and process images\n",
    "        image_tensors = []\n",
    "        for img_file in batch_image_files:\n",
    "            try:\n",
    "                image = Image.open(img_file).convert(\"RGB\")\n",
    "                image_tensors.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_file}: {e}\")\n",
    "\n",
    "        # Load and process captions\n",
    "        captions = []\n",
    "        for cap_file in batch_caption_files:\n",
    "            try:\n",
    "                with open(cap_file, 'r') as file:\n",
    "                    caption = file.read().strip()\n",
    "                captions.append(caption)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading caption {cap_file}: {e}\")\n",
    "\n",
    "        if len(image_tensors) == len(batch_image_files) and len(captions) == len(batch_caption_files):\n",
    "            inputs = processor(text=captions, images=image_tensors, return_tensors=\"pt\", padding=True)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
    "            text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            image_features_list.append(image_embeds.cpu())\n",
    "            text_features_list.append(text_embeds.cpu())\n",
    "        else:\n",
    "            print(\"Skipping batch due to errors in loading files.\")\n",
    "        \n",
    "        # Collect garbage to free up memory\n",
    "        gc.collect()\n",
    "\n",
    "    image_features = torch.cat(image_features_list, dim=0)\n",
    "    text_features = torch.cat(text_features_list, dim=0)\n",
    "    return image_features, text_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d662dcf9-27ac-4082-a070-6383c02b4aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(features):\n",
    "    return F.normalize(features, p=2, dim=-1)\n",
    "\n",
    "def find_best_captions(image_features, text_features, caption_files, batch_size=128):\n",
    "    # Normalize features\n",
    "    image_features = normalize_features(image_features)\n",
    "    text_features = normalize_features(text_features)\n",
    "\n",
    "    num_images = image_features.size(0)\n",
    "    num_texts = text_features.size(0)\n",
    "    \n",
    "    best_captions = []\n",
    "    for start in range(0, num_images, batch_size):\n",
    "        end = min(start + batch_size, num_images)\n",
    "        image_batch = image_features[start:end]\n",
    "\n",
    "        batch_best_caption_indices = None\n",
    "\n",
    "        for text_start in range(0, num_texts, batch_size):\n",
    "            text_end = min(text_start + batch_size, num_texts)\n",
    "            text_batch = text_features[text_start:text_end]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                cosine_similarities = F.cosine_similarity(\n",
    "                    image_batch.unsqueeze(1),\n",
    "                    text_batch.unsqueeze(0),\n",
    "                    dim=-1\n",
    "                )\n",
    "\n",
    "            current_best_indices = cosine_similarities.argmax(dim=1)\n",
    "            current_best_similarities = cosine_similarities[range(cosine_similarities.size(0)), current_best_indices]\n",
    "\n",
    "            if batch_best_caption_indices is None:\n",
    "                batch_best_caption_indices = current_best_indices + text_start\n",
    "                previous_best_similarities = current_best_similarities\n",
    "            else:\n",
    "                previous_best_similarities = F.cosine_similarity(\n",
    "                    image_batch.unsqueeze(1),\n",
    "                    text_features[batch_best_caption_indices].unsqueeze(0),\n",
    "                    dim=-1\n",
    "                ).squeeze(1)\n",
    "\n",
    "                better_indices = current_best_similarities > previous_best_similarities\n",
    "                batch_best_caption_indices = torch.where(better_indices, current_best_indices + text_start, batch_best_caption_indices)\n",
    "\n",
    "        batch_best_caption_indices_list = batch_best_caption_indices.view(-1).cpu().tolist()\n",
    "        best_captions += [open(caption_files[idx], 'r').read().strip() for idx in batch_best_caption_indices_list]\n",
    "\n",
    "    return best_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b869d-ee0f-475f-a63b-0efb1fe5e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size for processing\n",
    "batch_size = 8\n",
    "\n",
    "# Process each dataset and save the best captions\n",
    "for key, value in directories.items():\n",
    "    image_files, caption_files = dataset_files[key]\n",
    "    print(f\"Processing {key}: {len(image_files)} images, {len(caption_files)} captions\")\n",
    "\n",
    "    matched_image_files, matched_caption_files = get_matching_files(image_files, caption_files)\n",
    "    print(f\"Matched pairs: {len(matched_image_files)} images and {len(matched_caption_files)} captions\")\n",
    "\n",
    "    image_features, text_features = process_batches(matched_image_files, matched_caption_files, batch_size)\n",
    "    best_captions = find_best_captions(image_features, text_features, matched_caption_files, batch_size)\n",
    "    save_best_captions(best_captions, matched_image_files, value['output'])\n",
    "\n",
    "print(\"Best captions saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7334f2-126c-4d83-b5b9-9a4905993309",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
